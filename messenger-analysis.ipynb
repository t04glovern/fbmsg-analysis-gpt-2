{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## S3 Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "bucket='devopstar'\n",
    "data_key = 'resources/fbmsg-analysis-gpt-2/facebook.zip'\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "with open('facebook.zip', 'wb') as data:\n",
    "    s3.Bucket(bucket).download_fileobj(data_key, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip facebook.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh download_model.sh 117M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get List of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for p, d, f in os.walk('messages/inbox'):\n",
    "    for file in f:\n",
    "        if file.endswith('message.json'):\n",
    "            files.append(f'{p}/{file}')\n",
    "\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_encoding(s):\n",
    "    return re.sub('[\\xc2-\\xf4][\\x80-\\xbf]+',lambda m: m.group(0).encode('latin1').decode('utf8'),s)\n",
    "\n",
    "def find_cyrilic(s):\n",
    "    return len(re.findall('(?i)[А-ЯЁ]', s)) > 0\n",
    "\n",
    "def test_mostly_cyrilic(messages):\n",
    "    i = 0\n",
    "    check_n = min(250, len(messages))\n",
    "    for msg in random.sample(messages, check_n):\n",
    "        try:\n",
    "            i +=find_cyrilic(fix_encoding(msg['content'])) or find_cyrilic(fix_encoding(msg['sender_name']))\n",
    "        except KeyError:\n",
    "            check_n -=1\n",
    "    return i > check_n/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### All Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(files=files):\n",
    "    text_corpus = ''\n",
    "    banned_names = ()\n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            try:\n",
    "                msgs = json.load(f)['messages']\n",
    "                msgs.reverse()\n",
    "            except:\n",
    "                pass\n",
    "            else:\n",
    "                if not test_mostly_cyrilic(msgs) and not any(bn in file for bn in banned_names):\n",
    "                    for msg in msgs:\n",
    "                        try:\n",
    "                            content = fix_encoding(msg['content'])\n",
    "                            to_add  = f\"({msg['timestamp_ms']}) {msg['sender_name']}: {content}\\n\"\n",
    "                            if not find_cyrilic(to_add):\n",
    "                                text_corpus += to_add\n",
    "                        except KeyError:\n",
    "                            pass\n",
    "                    print(file)\n",
    "\n",
    "    text_corpus += '\\n\\n'\n",
    "    with open('fb-cleaned.txt', 'w') as f:\n",
    "          f.write(text_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Specify Particular Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_specific_file(person, files=files):\n",
    "    text_corpus = ''\n",
    "    for file in files:\n",
    "        if person in file:\n",
    "            print(file)\n",
    "            with open(file, 'r') as f:\n",
    "                try:\n",
    "                    msgs = json.load(f)['messages']\n",
    "                    msgs.reverse()\n",
    "                except:\n",
    "                    pass\n",
    "                else:\n",
    "                    for msg in msgs:\n",
    "                        try:\n",
    "                            content = fix_encoding(msg['content'])\n",
    "                            to_add  = f\"({msg['timestamp_ms']}) {msg['sender_name']}: {content}\\n\"\n",
    "                            if not find_cyrilic(to_add):\n",
    "                                text_corpus += to_add\n",
    "                        except KeyError:\n",
    "                            pass\n",
    "\n",
    "    text_corpus += '\\n\\n'\n",
    "    with open(f'fb-cleaned-{person}.txt', 'w') as f:\n",
    "        f.write(text_corpus)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!PYTHONPATH=src ./encode.py --in-text fb-cleaned.txt --out-npz fb-cleaned.txt.npz\n",
    "!PYTHONPATH=src ./train.py --dataset fb-cleaned.txt.npz --sample_every=250 --learning_rate=0.0001 --stop_after=251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv checkpoint/run1/* models/117M/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 src/generate_unconditional_samples.py --top_k 40 --temperature 0.9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
